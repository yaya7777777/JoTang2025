# Titanic 模型准确率优化实验记录

## 实验概述
本实验旨在通过系统性调整模型架构、超参数和训练策略，提高Titanic生存预测模型的准确率。每次调整将记录具体修改内容、训练结果和准确率变化

## 实验环境
- 框架：PyTorch
- 评估指标：准确率(Accuracy)和F1分数
- 硬件：CPU/CUDA 
  
## 实验记录格式
每次实验记录包括：
1. 实验编号
2. 实验日期时间
3. 具体修改内容
4. 训练结果(最佳验证准确率、F1分数等)
5. 分析与结论
  
## 实验记录

### 实验1: 基准模型
- **日期时间**: 2025-9-11
- **修改内容**: 原始基准模型
  - 网络架构: 128 → 64 → 32 → 16 → 2 (注意：第4层后没有激活函数)
  - Dropout: 0.3, 0.3, 0.2
  - Batch Size: 10
  - 优化器: AdamW (lr=0.0005, weight_decay=1e-5)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 200, 早停耐心值: 15
- **训练结果**: 
  - 最佳验证准确率: 0.8097
  - 最终评估准确率: 0.8022
  - F1分数: 0.7135
  - 训练轮数: 35 (因早停机制停止)
- **分析与结论**: 基准模型实现了约80%的准确率，模型在第35个epoch时因验证集性能不再提升而停止训练，可以尝试更改超参数来提高准确率

### 实验2: 扩大网络规模
- **日期时间**: 2025-9-11
- **修改内容**: 增加网络容量，扩大每层神经元数量
  - 网络架构: 256 → 128 → 64 → 32 → 2 (注意：第4层后没有激活函数)
  - Dropout: 0.3, 0.3, 0.2
  - Batch Size: 10
  - 优化器: AdamW (lr=0.0005, weight_decay=1e-5)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 200, 早停耐心值: 15
- **训练结果**: 
  - 最佳验证准确率: 0.8507
  - 最终评估准确率: 0.8209
  - F1分数: 0.7670
  - 训练轮数: 30 (因早停机制停止)
- **分析与结论**: 扩大网络规模后，模型性能有了显著提升，准确率从约80%提高到了82%，F1分数也有所提升，可见增加网络容量可以让模型学习到更复杂的特征表示，但训练轮数减少到30轮就停止了，可能还需要通过调整其他超参数进一步优化

### 实验3: 调整超参数
- **日期时间**: 2025-9-11
- **修改内容**: 在实验2的基础上调整多个超参数
  - 网络架构: 256 → 128 → 64 → 32 → 2 (注意：第4层后没有激活函数)
  - Dropout: 0.3, 0.3, 0.2
  - Batch Size: 32 (从10增加)
  - 优化器: AdamW (lr=0.001, weight_decay=5e-5) (学习率增加，权重衰减增加)
  - 学习率调度器: ReduceLROnPlateau (factor=0.5, patience=2) (更激进的学习率调整)
  - 训练轮数: 300, 早停耐心值: 20 (训练轮数和早停耐心值增加)
- **训练结果**: 
  - 最佳验证准确率: 0.8172
  - 最终评估准确率: 0.7948
  - F1分数: 0.7090
  - 训练轮数: 24 (因早停机制停止)
- **分析与结论**: 调整超参数后，模型性能反而有所下降，这可能是因为Batch Size从10增加到32导致模型学习不稳定，或者学习率过高（0.001）导致训练过程无法很好地收敛，所以需要进一步调整超参数组合

### 实验4: 降低学习率
- **日期时间**: 2025-9-11
- **修改内容**: 在实验3的基础上调整学习率相关参数
  - 网络架构: 256 → 128 → 64 → 32 → 2 (注意：第4层后没有激活函数)
  - Dropout: 0.3, 0.3, 0.2
  - Batch Size: 32
  - 优化器: AdamW (lr=0.0005, weight_decay=5e-5) (学习率从0.001降低到0.0005)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3) (恢复学习率调整策略)
  - 训练轮数: 300, 早停耐心值: 20
- **训练结果**: 
  - 最佳验证准确率: 0.8433
  - 最终评估准确率: 0.8321
  - F1分数: 0.7619
  - 训练轮数: 45 (因早停机制停止)
- **分析与结论**: 降低学习率并恢复更保守的学习率调度策略后，模型性能有了显著提升，准确率达到了83.21%，比实验3提高了约4个百分点。可见学习率是关键超参数，过高的学习率可能导致模型难以收敛，训练轮数也增加到了45轮，模型可能需要更多时间进行充分训练

### 实验5: 调整Dropout率
- **日期时间**: 2025-9-12
- **修改内容**: 在实验4的基础上增加Dropout率以减少过拟合
  - 网络架构: 256 → 128 → 64 → 32 → 2 (注意：第4层后没有激活函数)
  - Dropout: 0.4, 0.35, 0.3 (从0.3, 0.3, 0.2增加)
  - Batch Size: 32
  - 优化器: AdamW (lr=0.0005, weight_decay=5e-5)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 300, 早停耐心值: 20
- **训练结果**: 
  - 最佳验证准确率: 0.8134
  - 最终评估准确率: 0.8097
  - F1分数: 0.7358
  - 训练轮数: 28 (因早停机制停止)
- **分析与结论**: 增加Dropout率后，模型性能反而有所下降，准确率从83.21%下降到了80.97%，可能是因为Dropout率过高（特别是第一层的0.4）导致模型学习能力下降，需要尝试更合适的Dropout率

### 实验6: 微调Dropout率
- **日期时间**: 2025-9-12
- **修改内容**: 在实验5的基础上微调Dropout率为更平衡的值
  - 网络架构: 256 → 128 → 64 → 32 → 2 (注意：第4层后没有激活函数)
  - Dropout: 0.35, 0.3, 0.25 (从0.4, 0.35, 0.3略微降低)
  - Batch Size: 32
  - 优化器: AdamW (lr=0.0005, weight_decay=5e-5)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 300, 早停耐心值: 20
- **训练结果**: 
  - 最佳验证准确率: 0.8172
  - 最终评估准确率: 0.7910
  - F1分数: 0.7172
  - 训练轮数: 22 (因早停机制停止)
- **分析与结论**: 微调Dropout率后，模型性能仍然没有达到实验4的水平，当前的网络架构和超参数组合可能已经接近最佳状态，进一步的微调可能难以带来显著提升，接下来可以考虑其他优化方向，如特征工程或尝试不同的网络架构。

### 实验7: 增加网络层数并恢复最佳Dropout配置
- **日期时间**: 2025-9-12
- **修改内容**: 基于实验4的最佳配置，增加网络层数以增强表达能力
  - 网络架构: 256 → 128 → 64 → 32 → 16 → 2 (新增第5层，所有隐藏层后添加激活函数和Dropout)
  - Dropout: 0.3, 0.3, 0.2, 0.2 (恢复到实验4的最佳Dropout配置并扩展到新增层)
  - Batch Size: 32
  - 优化器: AdamW (lr=0.0005, weight_decay=5e-5)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 300, 早停耐心值: 20
- **训练结果**: 
  - 最佳验证准确率: 0.8321
  - 最终评估准确率: 0.8097
  - F1分数: 0.7671
  - 训练轮数: 33 (因早停机制停止)
- **分析与结论**: 增加网络层数并恢复到实验4的最佳Dropout配置后，模型表现略逊于实验4，但F1分数有所提升，增加网络层数并没有带来预期的性能提升，可能是因为数据集相对较小，更深的网络反而容易过拟合，实验4仍然是所有实验中的最佳配置

### 实验8: 更换优化器为SGD
- **日期时间**: 2025-9-12
- **修改内容**: 保持网络架构不变，更换优化器类型以探索不同的优化效果
  - 网络架构: 256 → 128 → 64 → 32 → 16 → 2
  - Dropout: 0.3, 0.3, 0.2, 0.2
  - Batch Size: 32
  - 优化器: SGD (lr=0.01, momentum=0.9, weight_decay=5e-5) (从AdamW更换为SGD，提高学习率并添加动量)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 300, 早停耐心值: 20
- **训练结果**: 
  - 最佳验证准确率: 0.8396
  - 最终评估准确率: 0.8321
  - F1分数: 0.7668
  - 训练轮数: 24 (因早停机制停止)
- **分析与结论**: 更换为SGD优化器并使用较大的学习率(0.01)和动量(0.9)后，模型取得了所有实验中最好的验证准确率(0.8396)，可见对于这个数据集，SGD优化器可能比AdamW更有效，学习率和动量的设置也很关键，SGD能够更有效地探索参数空间，特别是在有动量的情况下

### 实验9: 调整SGD优化器参数
- **日期时间**: 2025-9-12
- **修改内容**: 微调SGD优化器的参数以进一步提升性能
  - 网络架构: 256 → 128 → 64 → 32 → 16 → 2
  - Dropout: 0.3, 0.3, 0.2, 0.2
  - Batch Size: 32
  - 优化器: SGD (lr=0.005, momentum=0.95, weight_decay=5e-5) (降低学习率，增加动量)
  - 学习率调度器: ReduceLROnPlateau (factor=0.7, patience=3)
  - 训练轮数: 300, 早停耐心值: 20
- **训练结果**: 
  - 最佳验证准确率: 0.7836
  - 最终评估准确率: 0.7201
  - F1分数: 0.5665
  - 训练轮数: 24 (因早停机制停止)
- **分析与结论**: 降低学习率到0.005并增加动量到0.95后，模型性能显著下降，可见对于当前的网络架构和数据集，较大的学习率(0.01)和适中的动量(0.9)的组合更为有效，过高的动量可能导致模型在训练过程中跳过了最优解，而较低的学习率则可能导致收敛速度变慢或陷入局部最优，实验8的SGD参数配置仍然是最优的