### #概念理解


#### 从 RNN、LSTM、GRU 中选择两个，介绍其基本原理和优缺点

参考文献：
1.[如何从RNN起步，一步一步通俗理解LSTM](https://blog.csdn.net/v_JULY_v/article/details/89894058?sharetype=blog&shareId=89894058&sharerefer=APP&sharesource=2501_93331212&sharefrom=link)

2.[57 长短期记忆网络（LSTM）]( https://b23.tv/RAXKQfo)

3.[56 门控循环单元（GRU）]( https://b23.tv/4gR6bZy)


- LSTM
  - 基本原理：在RNN基础上引进了“门控机制”和“细胞状态”

  - 门控机制：三个门（遗忘门、输入门、输出门）来控制信息的流动，每个门都是一个Sigmoid神经网络层，输出0到1之间的值，表示“允许通过多少信息”

    - 遗忘门：决定从细胞状态中丢弃哪些信息

    - 输入门：决定哪些新信息将被存入细胞状态

    - 输出门：基于当前输入和隐藏状态，决定输出细胞状态的哪些部
分

  - 细胞状态 ：一条水平贯穿整个细胞的“传送带”，它能够相对无损地长距离传输信息，是模型记忆长期依赖的关键


    - 候选细胞状态：一个tanh层产生新的候选值

      - 更新细胞状态：将旧状态与遗忘门相乘，再加上输入门筛选过的候选值


      - 最终隐藏状态：将细胞状态通过tanh激活，并与输出门相乘

  - 优点：

    - 有效解决了RNN的梯度消失/爆炸问题，能够学习长期依赖关系

      - 门控机制提供了精细的信息流控制，性能强大

  - 缺点：

    - 参数较多，计算复杂，训练速度较慢

![alt text](f847ea4407d1eda878b3f05dc3e6be95-1.jpg)

- GRU
   
  - 基本原理：

    - GRU是LSTM的一种变体，它简化了结构，将LSTM的遗忘门和输入门合并为一个“更新门”，并混合了细胞状态和隐藏状态

      - 更新门：控制有多少过去的信息需要继续传递下去

      - 重置门：控制需要忽略多少过去的信息

      - 候选隐藏状态：使用重置门来控制过去状态的参与程度

      - 最终隐藏状态：是上一时刻隐藏状态和当前候选隐藏状态的加权和，权重由更新门控制

    - 优点：

      - 结构比LSTM更简单，参数更少，训练速度更快，在大多数任务上能达到与LSTM相当甚至更好的性能

    - 缺点：

      - 在某些需要极长期记忆的复杂任务上，性能可能略逊于LSTM

![alt text](0d006a80167840693cffd72d3cf336f6.jpg)
  

#### transformer

参考文献：1.[从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)]( https://b23.tv/pnH6E85)

2.[68 Transformer【动手学深度学习v2】]( https://b23.tv/H7uc2jg)

3.[【超详细】【原理篇&实战篇】一文读懂Transformer](https://blog.csdn.net/weixin_42475060/article/details/121101749?sharetype=blog&shareId=121101749&sharerefer=APP&sharesource=2501_93331212&sharefrom=link)

##### - 注意力机制的分类？现在有哪些最常用？

- 分类维度1：关注范围

  - 全局注意力（Global Attention）：标准注意力，计算当前词元与序列中所有其他词元的相关性

    - 局部注意力（Local/Masked Attention）：只计算当前词元与一个局部窗口内词元的相关性，降低计算复杂度

    - 因果掩码注意力（Causal Masked Attention）：解码器使用的掩码注意力，确保当前位置只能关注到之前的位置，不能“窥见未来”

- 分类维度2：计算方式

  - 点积注意力（Dot-Product）：最常用，计算Query和Key的点积来表示相关性

    - 加性注意力（Additive）：早期方案，通过一个前馈网络计算兼容性分数，计算更复杂

- 现在最常用：缩放点积注意力（Scaled Dot-Product Attention），即Transformer原论文中的标准形式，并结合因果掩码用于解码生成

##### - 什么是词向量？为什么我们需要用向量来表示词？

- 词向量（Word Embedding）：是将自然语言中的词语（符号）映射到实数域上的低维、稠密、连续的向量表示

- 为什么需要：

  - 计算需要：计算机无法直接处理文本符号，必须转化为数值形式

  - 蕴含语义：词向量能够在向量空间中捕捉词语的语义和语法关系，具有相似语义的

  - 稠密高效：相比于one-hot编码，它维度低且信息密集，计算效率高

![alt text](bc76f3f8347e49b7846271a49d9cf87a.jpeg)

![alt text](445652c36ab4badaba63439f3afa6bb0.jpeg)


##### - one-hot 编码是什么？为什么使用 one-hot 编码来表示词语会导致维度灾难？

- One-hot编码是一种最简单的词表示方法，假设词汇表大小为V，每个词用一个长度为V的向量表示，该词对应位置为1，其他所有位置为0

>例如：“猫” = [1, 0, 0, ..., 0],
“狗” = [0, 1, 0, ..., 0]

- 维度灾难（Curse of Dimensionality）：

  - 向量维度高：维度大小等于词汇表大小V（可达数万甚至百万），导致存储和计算开销巨大

  - 向量稀疏：每个向量中仅有1个1，其余全是0，有效信息极少

  - 无法表达语义关系：任意两个不同的词向量是正交的，点积为0，无法通过计算来衡量词与词之间的相似性

##### - Word2Vec 提供了哪两种训练架构？分别是如何工作的？

- Word2Vec通过“上下文预测词”或“词预测上下文”的方式来学习词向量

- CBOW (Continuous Bag-of-Words)

  - 工作原理：给定一个词的上下文，模型预测中心词本身

>例如:上下文是“今天 天气 很”，模型需要预测出中心词“好”

![alt text](d8fab75b3d16dc3589b09a4b46282ffc.jpeg)

- Skip-gram

  - 工作原理：与CBOW相反，给定一个中心词，模型预测其上下文词

>例如:中心词是“好”，模型需要预测出它周围的词“今天”、“天气”、“很”等

![alt text](7bda0a6e6c7b07d9abaf3da79d4ca666.jpeg)

- 两者的训练都是通过一个简单的神经网络（输入层、投影层、输出层）完成的，最终我们取隐藏层的权重或网络的输入权重作为训练好的词向量

#### 位置编码

##### - 为什么Transformer 本身没有任何位置意识？

- Transformer的核心是自注意力机制,而自注意力机制通过计算所有词元之间的相关性来生成新的表示
这个过程是置换不变的，也就是说，无论你如何打乱输入序列的顺序，自注意力机制都会为每个词元计算出与其他所有词元相同的相关性权重集合，从而产生相同的输出表示
它本身无法感知词与词之间的相对或绝对位置信息

##### - 位置编码可以是 绝对的（Absolute）或 相对的（Relative），还可以是可学习的（Learnable），他们分别是什么、有什么优劣？

- 绝对位置编码 (Absolute)：

  - 是什么：为序列中的每个位置分配一个独一无二的、固定的编码向量
    最常见的是Transformer原论文中的正弦余弦编码，使用不同频率的正弦和余弦函数来生成编码

  - 优点：确定的，无需学习；可以外推到比训练序列更长的序列

  - 缺点：是固定的，可能无法完美适配特定任务的数据分布

![alt text](af82c0c7d1eedb883341bc2d0f12bb84.jpg)

- 相对位置编码 (Relative)：

  - 是什么：不关注词元的绝对位置，而是关注词元之间的相对距离（如“相邻”、“隔一个”），它通常在计算Attention Score时注入相对位置信息

  - 优点：更好地建模元素间的相对关系，通常具有更好的泛化能力和性能

  - 缺点：实现相对复杂，计算开销可能稍大

![alt text](4feb56a7e9ae32903f7bb7dc1ffe5965-1.jpeg)
  
- 可学习的位置编码 (Learnable)：

  - 是什么：将位置编码视为模型参数，随机初始化后随训练过程一起学习

  - 优点：灵活，可以自适应地学习到最适合当前任务的位置模式。

  - 缺点：只能处理在训练时见过的序列长度，泛化到更长的序列比较困难
  
##### - 大语言模型（LLM）中常用的位置编码有哪些？

- LiBi (Attention with Linear Biases)：一种相对位置编码，通过在Attention Score上添加一个与相对距离成负比的线性偏置来实现
泛化到更长序列的能力极强，被用于Bloom、MPT等模型

- Rotary Position Embedding (RoPE)：旋转位置编码，通过旋转矩阵对Query和Key向量进行变换，将绝对位置信息巧妙地融入其中，同时自然产生相对位置依赖
是目前最主流的方案，被用于LLaMA、PaLM、ChatGLM等绝大多数主流模型

- T5's Relative Bias：在计算Attention时，根据Query和Key的相对距离，从一个可学习的嵌入表中查找一个标量偏置值加到Attention Score上
一般用于T5模型
 
#### 层归一化 LayerNorm

##### - 什么是 Layer Normalization？为什么需要它？

- 是什么：LayerNorm是一种深度学习中的归一化技术
它对单个样本的所有特征（或一个序列中所有词元的特征）进行标准化，使其均值为0，方差为1，并引入可学习的缩放和偏移参数

- 公式： $LN(x) = γ * (x - μ) / √(σ² + ε) + β$ ，其中μ和σ是沿特征维度计算的均值和方差

- 为什么需要：

  - 稳定训练，加速收敛：缓解深度网络中的内部协变量偏移问题，即每层输入分布随着前一层参数更新而发生变化的问题,通过规范化，使每层的输入分布更稳定。

  - 缓解梯度消失/爆炸：将激活值scale到合理范围，有利于梯度传播

  - 对批次大小不敏感：特别适用于批次大小很小或序列长度变化大的场景

##### - LayerNorm 和 BatchNorm 的区别？

| 特性               | LayerNorm (LN)                          | BatchNorm (BN)                          |
|--------------------|-----------------------------------------|-----------------------------------------|
| **归一化维度**     | **同一样本内**的所有特征（H, W, C）     | **同一特征** across **批次中所有样本**（N） |
| **数据依赖**       | 不依赖于批次大小，**对批次大小不敏感**  | 严重依赖于批次统计量，**小批次效果差**    |
| **适用领域**       | **自然语言处理（NLP）、循环神经网络（RNN）** | **计算机视觉（CV）、卷积神经网络（CNN）** |
| **训练/推理**      | 行为一致，直接计算当前样本的统计量      | 训练时用批次统计量，推理时用固定 running mean/var |


![alt text](5763d2183b13d3c903ff3f1bb16a9bda.jpeg)

#### mask

##### - 什么是 Causal Mask / Look-ahead Mask？作用是什么？

- 是什么：Causal Mask（因果掩码）或 Look-ahead Mask（前瞻掩码）是一种应用于Transformer解码器自注意力层的上三角矩阵（主对角线及以下为0，以上为负无穷大-inf）。

- 作用：

  - 保证自回归特性：在生成词元时，模型只能“看到”和“关注”之前已经生成的词元，而无法“窥见”未来的词元

  - 防止数据泄露：在训练解码器时，如果不使用掩码，模型会直接看到整个目标序列的答案，导致它简单地复制后续词元，无法学会真正的序列生成

![alt text](7fb140db2932d5d983a97eec1be4978e.jpeg)

#### 束搜索

##### - 为什么 LLM 输出序列是一个搜索问题（每次输出一个概率分布列）？

- LLM（大语言模型）是自回归的，即每次根据已有的上文，预测下一个词的概率分布（$ P(y_t | y_<t, X)$ ），然后从这个分布中采样或取最可能的词，再将新生成的词作为新的输入，继续预测下一个词，循环往复直至生成结束符

- **为什么是搜索问题**：
生成整个序列 $Y = (y_1, y_2, ..., y_T)$ 的概率是每一步条件概率的连乘： $P(Y|X) = Π P(y_t | y_<t, X)$ 
我们的目标是找到概率乘积最大（即整体最合理、最流畅）的输出序列,由于词汇表很大，序列长度可能很长,可能的序列组合是指数级增长的
我们无法遍历所有可能的序列，因此必须使用启发式搜索算法来在巨大的搜索空间中高效地找到一个近似最优解

- **常用搜索算法**：

  - 贪心搜索 (Greedy Search)：每一步都选择概率最大的词
  缺点：容易错过整体更优但局部概率非最大的序列
  >如:“A-B”概率0.36，优于“C-D”0.3，但第一步 greedy 会选C

  ![alt text](b4d124f5a17b9be2c987d3594482540a.jpeg)

  - 束搜索 (Beam Search)：是贪婪搜索的扩展，每一步保留概率最大的 k 个候选序列（ k 称为束宽）
  优点：在计算开销和搜索结果质量之间取得了很好的平衡，是之前序列生成任务（如机器翻译）的标准方法

  ![alt text](58fddf367ff1ac50848347beb6901eb7.jpeg)

  - 采样 (Sampling)：如温度采样、Top-k采样、Top-p采样，通过引入随机性来生成多样化的文本，是现代对话式LLM更常用的方法

#### BERT

参考文献：1.[BERT 论文逐段精读【论文精读】]( https://b23.tv/oyd5nIz)

2.[一文读懂BERT(原理篇) ](https://blog.csdn.net/jiaowoshouzi/article/details/89073944?sharetype=blog&shareId=89073944&sharerefer=APP&sharesource=2501_93331212&sharefrom=link)

##### - 解释 BERT 的架构和原理？
- BERT（Bidirectional Encoder Representations from Transformers）的本质是一个****基于Transformer编码器堆叠**的预训练语言模型

- 架构：BERT-base由12层Transformer编码器堆叠而成
  它只使用了Transformer的编码器部分，因为编码器设计用于对整个输入序列进行****双向**编码

- 原理：BERT的核心思想是通过无监督学习，在海量无标注文本上进行**预训练**，学习通用的语言表示
- 预训练完成后，可以通过添加一个简单的输出层来**微调**，以适应各种下游任务
##### - 为什么说 BERT 是“双向的”？

- 传统的语言模型（如GPT）是单向的（**从左到右或从右到左**），每个词只能关注到它之前的上文

- BERT在预训练阶段使用了掩码语言模型（MLM） 任务，随机遮盖输入序列中的一些词，然后让模型根据上下文中的所有词（被遮盖词的左右两侧的词） 来预测被遮盖的词
这种利用**完整上下文信息**进行预测的方式，就是其“双向”的含义
它的自注意力机制是全局的，不受方向限制。

##### - BERT 的预训练任务是什么？

参考文献：1.[69 BERT预训练【动手学深度学习v2】]( https://b23.tv/RK1m7Gn)

2.[70 BERT微调【动手学深度学习v2】]( https://b23.tv/SD4oWCL)



- BERT同时使用两个预训练任务：

  - 掩码语言模型 (MLM)：随机遮盖15%的输入词元
    其中80%替换为 [MASK] ，10%替换为随机词，10%保持不变
    模型的任务是预测这些被遮盖位置的原始词。

  - 下一句预测 (NSP)：给定两个句子A和B，模型预测句子B是否是句子A的下一句
    输入格式为：[CLS] A [SEP] B [SEP] 
    任务是一个二分类问题，结果是isnext或notnext，目的是让模型理解句子间的关系

##### - BERT 的输入嵌入（Input Embedding）是由哪三部分相加而成的？为什么需要这三部分？

- 词嵌入 (Token Embedding)：将每个词转换为向量表示

- 段嵌入 (Segment Embedding)：用于区分和标记句子对
  >例如:句子A的所有词元都使用嵌入$ E_A$ ，句子B的所有词元都使用嵌入$ E_B$ 

- 位置嵌入 (Position Embedding)：为每个词元的位置信息提供编码，是可学习的（与Transformer原论文的正弦编码不同）

- 为什么需要：

  - 词嵌入：提供词汇的语义信息

  - 段嵌入：让模型能够区分两个不同的句子，这对NSP任务和理解句子对关系至关重要

  - 位置嵌入：为模型提供序列的顺序信息，弥补Transformer结构本身不具备位置感知的缺陷

##### - BERT 和 Transformer 有何区别？

| 特性               | Transformer                             | BERT                                    |
|--------------------|-----------------------------------------|-----------------------------------------|
| **架构**           | 一个**完整的Seq2Seq模型**，包含**编码器（Encoder）和解码器（Decoder）** 两部分 | **仅使用Transformer的编码器（Encoder）** 部分堆叠而成 |
| **注意力机制**     | 编码器使用**全局注意力**，解码器使用**带掩码的全局注意力**（因果掩码） | 使用**全局双向自注意力** |
| **训练方式**       | 通常在一个特定的Seq2Seq任务（如机器翻译）上**端到端训练** | 先在无标签数据上进行**自监督预训练（MLM+NSP）**，再在下游任务上**微调** |
| **应用**           | 直接用于**序列到序列的生成任务**，如翻译、摘要。 | 主要用于**语言理解任务**（分类、问答、抽取），**不直接用于文本生成** |



---

### #代码实践 GLUE/SST-2（句子级情感，二分类）

首先我们要先下载数据库(其中包含训练集、验证集和测试集)

OK从ai处了解到这个数据在datasets的库里

![alt text](QQ_1758268943561.png)

这是一个二分类的库，标签中0代表负面，1代表正面

然后加载词向量

大抵是一个降维映射？看了视频感觉是把许多维度的特征投射到空间中一个点，达到降维且具有相关语义的目的

bert-base-uncased作为分词器获得子词tokens
uncased将单词转化为小写

（查了一下如果用中文数据集ChnSentiCorp的话只需要将分词器改为bert-base-chinese）

![alt text](QQ_1758276960929.png)

输入参数examples是数据集的一个batch
truncation=True是超过max_length的文本会被截断
max_length=128设置最大长度为128
padding="max_length"代表不足128的会用[PAD]补齐
返回包含input_ids, attention_mask等的字典（会自动标记真实文本和补齐（大概是为了区分

![alt text](QQ_1758277462674.png)

np.argmax是为了获取概率最大的类别作为预测结果
计算accuracy和binary f1分数作为指标

num_labels=2表示二分类任务，会自动在BERT基础上添加分类头

![alt text](QQ_1758280321063.png)

调用函数进行训练

按照要求保存参数和模型

![alt text](QQ_1758280520240.png)

哦忘了随机种子，补上补上
（其实我一直疑惑为什么默认都是42，按理来说，随机种子是不是填什么数值都一样？）

![alt text](QQ_1758281540549.png)

运行一下子


![alt text](bf8bdceb8ada8c086d2a69f1a1a9c359.png)

我们需要保存的东西都在这里了
话说我有保存这么多吗

---

### #ideas&questions

- ai说用了train（）可以自动进行前向传播、计算损失、逆向传播这一套
  那我之前为什么都要写出来？？因为我傻吗

  （OK现在我查了，因为这个模型![alt text](QQ_1758282078443.png)里含有完整的神经网络和自动微分机制，太香了）

- 关于随机种子我还是没太懂来着，感觉像是固定无关变量，那这些数值代表什么呢？
  可以不可以说设置随机种子就是为了颗粒度对齐？

- 为啥有的保存成了.json,有的成了.bin，有的成了.txt，这玩意儿是可以自己判断然后决定的吗？不需要我自己设置？
  anyway，那些个警告我不要打开，打开就是一堆乱码的保存，在哪里可以正常观看啊？

- #### 都做到这里来了，就特别迷茫，感觉基本上什么都是预制品，我做的事就是去搜我要干什么，然后网上就会有一大堆模板化代码，甚至很多都只需要调用库里的函数就可以一行带过，中间的具体转化都不清楚，或者说理解了也不会应用，我就只用将路径啊参数啊变量啊进行修改，增加一些可视化一些保存
  #### 导致我现在对原理比较清楚，知道我要干什么，但是完全背不下来各种各样的函数，而且也不知道这些函数属于哪些库，没有独立写代码的能力，这对吗？？













